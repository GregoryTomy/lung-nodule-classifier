{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_output = pd.read_csv(\"val_out_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.162903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75493</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.729822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75494</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.376483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.359873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75498 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Predicted\n",
       "0         0.0   0.096034\n",
       "1         0.0   0.011547\n",
       "2         0.0   0.031095\n",
       "3         0.0   0.078262\n",
       "4         0.0   0.162903\n",
       "...       ...        ...\n",
       "75493     0.0   0.729822\n",
       "75494     0.0   0.376483\n",
       "75495     0.0   0.017373\n",
       "75496     0.0   0.002516\n",
       "75497     0.0   0.359873\n",
       "\n",
       "[75498 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fp_per_scan = 8\n",
    "num_thresholds = int(max_fp_per_scan * 16) + 1 # for 1/16th increments\n",
    "thresholds = np.linspace(0, max_fp_per_scan, num_thresholds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'val_output' is a DataFrame with 'Actual' and 'Predicted' columns\n",
    "# Sort predictions by their score in descending order\n",
    "sorted_predictions = val_output[\"Predicted\"].sort_values(ascending=False)\n",
    "sorted_labels = val_output[\"Actual\"].loc[sorted_predictions.index]\n",
    "\n",
    "# Calculate cumulative false positives and true positives\n",
    "cumulative_false_positives = np.cumsum(1 - sorted_labels.values)\n",
    "cumulative_true_positives = np.cumsum(sorted_labels.values)\n",
    "\n",
    "# Total positives for sensitivity calculation and total scans\n",
    "total_positives = sorted_labels.sum()\n",
    "total_scans = len(val_output)\n",
    "\n",
    "# Initialize arrays for FROC curve\n",
    "thresholds = np.linspace(0, max_fp_per_scan, int(max_fp_per_scan * 4) + 1)\n",
    "sensitivities = []\n",
    "fp_per_scan = []\n",
    "\n",
    "# Calculate sensitivity and false positives per scan for each threshold\n",
    "for avg_fp_per_scan in thresholds:\n",
    "    num_fp_allowed = int(avg_fp_per_scan * total_scans)\n",
    "    # Get the largest index where cumulative false positives is less than or equal to num_fp_allowed\n",
    "    threshold_index = np.where(cumulative_false_positives <= num_fp_allowed)[0][-1]\n",
    "    \n",
    "    # Sensitivity: ratio of true positives at threshold_index to total positives\n",
    "    sensitivity = cumulative_true_positives[threshold_index] / total_positives\n",
    "    sensitivities.append(sensitivity)\n",
    "    \n",
    "    # Average number of false positives per scan at the current threshold\n",
    "    avg_fp = cumulative_false_positives[threshold_index] / total_scans\n",
    "    fp_per_scan.append(avg_fp)\n",
    "\n",
    "# Plot the FROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, sensitivities, color='#c8af76',lw=2, marker='D', markersize=6)\n",
    "plt.xlabel('Average Number of False Positives per Scan', fontsize=16)\n",
    "plt.ylabel('Sensitivity', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14, length=6, width=2)\n",
    "plt.tick_params(axis='both', which='minor', length=4, width=1)\n",
    "plt.gca().spines['top'].set_linewidth(1.5)\n",
    "plt.gca().spines['right'].set_linewidth(1.5)\n",
    "plt.gca().spines['bottom'].set_linewidth(1.5)\n",
    "plt.gca().spines['left'].set_linewidth(1.5)\n",
    "plt.grid(which=\"both\", linestyle=\"-\", linewidth=0.5, color=\"grey\", alpha=0.5)\n",
    "plt.savefig(\"images/froc.jpg\", bbox_inches='tight', dpi=1200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_froc(model_output, allowed_distance, num_thresholds=40):\n",
    "    # Convert the model output to a DataFrame if it's not already\n",
    "    if not isinstance(model_output, pd.DataFrame):\n",
    "        model_output = pd.DataFrame(model_output, columns=['Actual', 'Predicted'])\n",
    "\n",
    "    # Initialize lists to store FROC data\n",
    "    sensitivity_list = []\n",
    "    fp_avg_list = []\n",
    "    threshold_list = np.linspace(model_output['Predicted'].min(), model_output['Predicted'].max(), num_thresholds)\n",
    "\n",
    "    for threshold in threshold_list:\n",
    "        # Apply threshold\n",
    "        predictions = model_output['Predicted'] >= threshold\n",
    "\n",
    "        # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "        TP = ((model_output['Actual'] == 1) & predictions).sum()\n",
    "        FP = ((model_output['Actual'] == 0) & predictions).sum()\n",
    "        FN = ((model_output['Actual'] == 1) & ~predictions).sum()\n",
    "        P = model_output['Actual'].sum() # Total number of actual positives\n",
    "\n",
    "        # Calculate Sensitivity and False Positives per Image\n",
    "        sensitivity = TP / P if P > 0 else 0\n",
    "        fp_avg = FP / len(model_output) # Assuming each row is an 'image'\n",
    "\n",
    "        # Append to lists\n",
    "        sensitivity_list.append(sensitivity)\n",
    "        fp_avg_list.append(fp_avg)\n",
    "\n",
    "    return sensitivity_list, fp_avg_list, threshold_list\n",
    "\n",
    "# Example Usage\n",
    "model_output = val_output\n",
    "sensitivity, fp_avg, thresholds = calculate_froc(model_output, allowed_distance=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, sensitivity, color='#c8af76',lw=2, marker='D', markersize=6)\n",
    "plt.xlabel('Average Number of False Positives per Scan', fontsize=16)\n",
    "plt.ylabel('Sensitivity', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14, length=6, width=2)\n",
    "plt.tick_params(axis='both', which='minor', length=4, width=1)\n",
    "plt.gca().spines['top'].set_linewidth(1.5)\n",
    "plt.gca().spines['right'].set_linewidth(1.5)\n",
    "plt.gca().spines['bottom'].set_linewidth(1.5)\n",
    "plt.gca().spines['left'].set_linewidth(1.5)\n",
    "plt.grid(which=\"both\", linestyle=\"-\", linewidth=0.5, color=\"grey\", alpha=0.5)\n",
    "# plt.savefig(\"images/froc.jpg\", bbox_inches='tight', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9979337200985456,\n",
       " 0.5789159977747755,\n",
       " 0.40651407984317467,\n",
       " 0.3214389785159872,\n",
       " 0.27039126864287794,\n",
       " 0.23437706959124746,\n",
       " 0.20763463932819412,\n",
       " 0.18548835730747834,\n",
       " 0.16747463508967125,\n",
       " 0.15222919812445362,\n",
       " 0.13983151871572758,\n",
       " 0.12817558081008767,\n",
       " 0.11828127897427747,\n",
       " 0.10936713555325969,\n",
       " 0.10173779437865904,\n",
       " 0.09465151394738934,\n",
       " 0.08837320193912422,\n",
       " 0.08247900606638586,\n",
       " 0.076730509417468,\n",
       " 0.07153831889586479,\n",
       " 0.06651831836604943,\n",
       " 0.06194866089167925,\n",
       " 0.05785583724072161,\n",
       " 0.0533788974542372,\n",
       " 0.049484754563034784,\n",
       " 0.04567008397573446,\n",
       " 0.042371983363797716,\n",
       " 0.038424858936660575,\n",
       " 0.035007549868870697,\n",
       " 0.03177567617685237,\n",
       " 0.028318630957111447,\n",
       " 0.025470873400619883,\n",
       " 0.02245092585234046,\n",
       " 0.01940448753609367,\n",
       " 0.01662295689952052,\n",
       " 0.013920898566849454,\n",
       " 0.011258576386129433,\n",
       " 0.008000211926143738,\n",
       " 0.004702111314206999,\n",
       " 0.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_froc_score(model_output, num_images, num_thresholds=1000):\n",
    "    if not isinstance(model_output, pd.DataFrame):\n",
    "        model_output = pd.DataFrame(model_output, columns=['Actual', 'Predicted'])\n",
    "\n",
    "    threshold_list = np.linspace(model_output['Predicted'].min(), model_output['Predicted'].max(), num_thresholds)\n",
    "    predefined_fp_rates = [1/8, 1/4, 1/2, 1, 2, 4, 8]  # Predefined false positive rates per scan\n",
    "    sensitivity_at_predefined_fp_rates = []\n",
    "\n",
    "    for fp_rate in predefined_fp_rates:\n",
    "        max_sensitivity = 0\n",
    "        for threshold in threshold_list:\n",
    "            # Apply threshold\n",
    "            predictions = model_output['Predicted'] >= threshold\n",
    "\n",
    "            # Calculate TP, FP\n",
    "            TP = ((model_output['Actual'] == 1) & predictions).sum()\n",
    "            FP = ((model_output['Actual'] == 0) & predictions).sum()\n",
    "            P = model_output['Actual'].sum()  # Total actual positives\n",
    "\n",
    "            # Calculate Sensitivity and FP per Image\n",
    "            sensitivity = TP / P if P > 0 else 0\n",
    "            fp_per_image = FP / num_images\n",
    "\n",
    "            # Check if FP per image is close to the predefined rate\n",
    "            if fp_per_image <= fp_rate:\n",
    "                max_sensitivity = max(max_sensitivity, sensitivity)\n",
    "\n",
    "        sensitivity_at_predefined_fp_rates.append(max_sensitivity)\n",
    "\n",
    "    # Calculate the final FROC score as the average sensitivity at predefined FP rates\n",
    "    froc_score = np.mean(sensitivity_at_predefined_fp_rates)\n",
    "    return froc_score, sensitivity_at_predefined_fp_rates, predefined_fp_rates\n",
    "\n",
    "# Example Usage\n",
    "model_output = val_output \n",
    "num_images =  len(val_output)\n",
    "froc_score, sensitivities, fp_rates = calculate_froc_score(model_output, num_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9926739926739927"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "froc_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
